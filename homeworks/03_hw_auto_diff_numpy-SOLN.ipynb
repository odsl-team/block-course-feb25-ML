{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288a23c5-4388-45b4-94ab-f0af2088d824",
   "metadata": {},
   "source": [
    "# Homework 3: Auto diff in numpy\n",
    "\n",
    "Iterations on a classical theme...\n",
    "\n",
    "In tutorials we've coded up networks in pytorch, julia... and this is one last exercise!\n",
    "However, since we have coded up custom pullbacks in julia already, if you'd like to answer the questions in this notebook by submitting a simple julia script instead, we'll also accept that solution!\n",
    "\n",
    "The goal of this exercise is just to cross check your work from pencil and paper math, to what you code up yourself, to some AD package :)\n",
    "\n",
    "**Table of Contents**\n",
    "1. Computational graphs \n",
    "2. Build a NN\n",
    "3. Calc NN gradients\n",
    "4. Check gradients in pytorch\n",
    "5. Add a loss ✏️ \n",
    "\n",
    "Nicole Hartman\n",
    "\n",
    "ODSL ML Block Course \n",
    "\n",
    "Originally from Apr 2024, last revised Feb 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab3ec40-2f2c-4e71-b96c-c5b6380e076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5a566-c2bf-4d72-a60c-98b4a33d6883",
   "metadata": {},
   "source": [
    "## Step 1: Computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1e4d5-6731-4583-b71a-6cdd3ae3d227",
   "metadata": {},
   "source": [
    "**Motivating example:**  Let's code up the model we had worked out in lecture.\n",
    "\n",
    "$$f(x,y,z) = (x+y) \\cdot z$$\n",
    "\n",
    "<span style=\"color:red\">$$f_1(x,y) = q = x+y$$</span>\n",
    "\n",
    "<span style=\"color:blue\">$$f_2(q,z) = q \\cdot z$$</span>\n",
    "\n",
    "<img src=\"../figures/toy-ex.jpg\" width=650>\n",
    "\n",
    "**To Do:** Implement `f1`, `f2` and `f` functions in numpy with both a forward and a backward mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bf1022aa-71a4-4ab1-a4fd-9b5f542549d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x,y, grad=None):\n",
    "    '''\n",
    "    f1(x,y) = x+y\n",
    "    \n",
    "    Inputs:\n",
    "    - x,y: The inputs to the functoin\n",
    "    - grad: if not None, this is the \"upstream gradient\"\n",
    "\n",
    "    Outpus:\n",
    "    - If (upstream) grad is passed, return f1(x,y), output_grad\n",
    "    - Else, just the \"normal\" fct, return f1(x,y)\n",
    "    '''\n",
    "    \n",
    "    out = x+y\n",
    "    if grad:\n",
    "        dqdx = 1\n",
    "        dqdy = 1\n",
    "        # here grad = df/dq\n",
    "        dfdx = dqdx * grad\n",
    "        dfdy = dqdy * grad\n",
    "        return out, np.array([dfdx, dfdy])\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0e4b3e8-116e-4727-81d8-69e2843d35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(q,z, grad=None):\n",
    "    '''\n",
    "    f2(q,z) = q*z\n",
    "    '''\n",
    "\n",
    "    out = q * z\n",
    "    if grad:\n",
    "       return out, np.array([z,q])*grad\n",
    "    else: \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5941b7b7-de97-49f8-992e-672758648f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y,z, grad=None):\n",
    "    '''\n",
    "    f(x,y,z) = f2(q,x)\n",
    "    '''\n",
    "\n",
    "    # Need to run the program 1x to get the values to eval the grad\n",
    "    \n",
    "    q = f1(x,y)\n",
    "    out = f2(q,z)\n",
    "    \n",
    "    if grad:\n",
    "        \n",
    "        _, [dfdq, dfdz] = f2(q,z,grad=grad)\n",
    "        _, [dfdx,dfdy] = f1(x,y, dfdq)\n",
    "        \n",
    "        return out, [dfdx, dfdy, dfdz]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1e2a2-a2fe-4a67-8997-1052be88420e",
   "metadata": {},
   "source": [
    "Check the forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fce6e1be-3907-4442-b162-33a1e56f5e74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12\n"
     ]
    }
   ],
   "source": [
    "x,y,z = 5,-2,4\n",
    "print(f(x,y,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf31f60-adf1-4e51-bea5-087feaafc3dc",
   "metadata": {},
   "source": [
    "\n",
    "**What about the derivative?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f12d235d-33a6-40a6-9936-f28a361bb8b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df/dx= 4\n",
      "df/dy= 4\n",
      "df/dz= 3\n"
     ]
    }
   ],
   "source": [
    "dfdf = 1\n",
    "out, grad = f(x,y,z, grad=dfdf)\n",
    "\n",
    "for xi, dfi in zip(['x','y','z'],grad):\n",
    "    print(f'df/d{xi}= {dfi}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26e93c0-1ecb-4b65-afdb-e39cebaadcfb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "32d4cf94-1812-4b1e-bdce-8c96b0f9c021",
   "metadata": {},
   "source": [
    "## Step 2: Multi-layer perceptron (forward pass)\n",
    "\n",
    "Define a simple feed-forward NN that accepts a input matrix\n",
    "\n",
    "$$X \\in \\mathbb{R}^{N \\times d}$$\n",
    "\n",
    "(where for the example just above $N=200$, $d=5$)\n",
    "\n",
    "and constructs an MLP with a single hidden layer with $H = 16$ units, and a ReLU non-linearity.\n",
    "\n",
    "**Hint:** this is just three equations:\n",
    "\n",
    "$$z = X W_1^T + b_1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c20e1-2788-41e3-bb41-555cc7931d22",
   "metadata": {},
   "source": [
    "$$h = \\mathrm{ReLU}(z_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cdfbf-0eea-4034-95cf-2cc45e6518a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "$$y_\\textrm{pred} =h W_2^T  + b_2$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{H \\times d}, b_1 \\in \\mathbb{R}^{H}, W_2 \\in \\mathbb{R}^{1 \\times H}, b_2 \\in \\mathbb{R}$,\n",
    "$z \\in \\mathbb{R}^{N\\times H}$, $h \\in \\mathbb{R}^{N \\times H}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fdd7f-9696-4240-bdcc-5d3286eadf3e",
   "metadata": {},
   "source": [
    "**Potentially useful functions:** \n",
    "- `np.einsum`\n",
    "- `np.where`\n",
    "- `np.random.randn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eeb3481b-5125-45cc-a9fc-db00b854b840",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll just use a random dataset for this problem\n",
    "n = 128\n",
    "d = 4\n",
    "X = np.random.randn(n,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1dd4b75b-c2a8-48e2-ab9f-c1b460ccf765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 4)\n",
      "(16,)\n",
      "(1, 16)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parameters (randomly) to have the desired shape\n",
    "H = 16\n",
    "\n",
    "'''\n",
    "TO DO: Initialize the parameters\n",
    "'''\n",
    "\n",
    "# 1st layer of the NN\n",
    "W1 = np.random.randn(H, d)\n",
    "b1 = np.random.randn(H)\n",
    "\n",
    "# Now the 2nd layer of the NN\n",
    "W2 = np.random.randn(1,H)\n",
    "b2 = np.random.randn(1)\n",
    "\n",
    "for v in [W1,b1,W2,b2]:\n",
    "    print(v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab05ccd-f9af-4df1-b66a-cabcd937c219",
   "metadata": {},
   "source": [
    "$$ReLU(x) = \\max(x,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b5e09c8-11f3-4898-bf71-efa74a168335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x>0, x, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9f91caa6-1558-4cda-bf1b-b6660d98f13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([1,3,-1])\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0141bf7a-7df2-4c01-aa04-2c07baf64be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 16)\n",
      "(128, 16)\n",
      "(128, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up the feedforward model\n",
    "\n",
    "# z = \"W1x + b1\"\n",
    "z = np.einsum('hj,ij->ih',W1,X) + b1\n",
    "print(z.shape)\n",
    "\n",
    "h = relu(z)\n",
    "print(h.shape)\n",
    "y_pred = np.einsum('oh,ih->io',W2,h) + b2\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40a3e3-39c1-4b92-bd02-ad7c3bb5b1c5",
   "metadata": {},
   "source": [
    "We're thinking about a regression task right now, so no need to have a non-linearity on the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be087011-6d57-496b-8fa5-9f68ba3c7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Does the output have the dimensionality we expect?\n",
    "assert y_pred.shape[0] == n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73adc084-eb39-4831-a7d5-2d36b60002f2",
   "metadata": {},
   "source": [
    "## Step 3: Gradients of a NN\n",
    "\n",
    "OK, let's re-code our \"neural network\" up, but this time with the gradient computation so we can do _back prop_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d6ea1e0c-4b15-4715-b0d4-fdc9be7d8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(W,X,b, grad=None):\n",
    "    '''\n",
    "    Linear layer mapping H = W x + b\n",
    "    - W: array (m,n)\n",
    "    - x: array (bs, n)\n",
    "    - b: array (m)\n",
    "\n",
    "    Outputs:\n",
    "    - H: array (bs,m)\n",
    "\n",
    "    If grad is an array (bs,m), return tuple: out, (grad_W, grad_b, grad_H)\n",
    "    - grad_W: array (bs, m, n)\n",
    "    - grad_b: array (bs, m)\n",
    "    - grad_X: array (bs,    n)\n",
    "    '''\n",
    "\n",
    "    assert X.shape[1] == W.shape[1]\n",
    "    assert W.shape[0] == b.shape[0]\n",
    "\n",
    "    H = np.einsum('mn,bn->bm',W,X) + b\n",
    "    \n",
    "    if grad is not None:\n",
    "        \n",
    "        grad_W = np.einsum('bn,bm->bmn',X,grad)\n",
    "        grad_b = grad * np.ones_like(b)\n",
    "        grad_X = np.einsum('mn,bm->bn',W,grad)\n",
    "        \n",
    "        return H, (grad_W, grad_b, grad_X)\n",
    "    else:\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "849c6d4b-25b8-45f0-9d63-4ea1ac07f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z,grad=None):\n",
    "    '''\n",
    "    Non-linearity: better gradient flow\n",
    "    '''\n",
    "    out = np.where(z>0,z,0)\n",
    "\n",
    "    if grad is not None:\n",
    "        dz = (z>0).astype(float) \n",
    "        return out, dz * grad\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2ce982f0-1f53-433e-bbd2-9c73a88786c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myNN(X, param_dict, grad=None):\n",
    "    '''\n",
    "    Build an MLP with a single hidden layer.\n",
    "    '''\n",
    "\n",
    "    # Unpack the parameter dict\n",
    "    W1, b1 = param_dict[\"W1\"], param_dict[\"b1\"]\n",
    "    W2, b2 = param_dict[\"W2\"], param_dict[\"b2\"]\n",
    "\n",
    "    # forward pass\n",
    "    z = linear(W1,X,b1)\n",
    "    h = relu(z)\n",
    "    out = linear(W2,h,b2)\n",
    "\n",
    "    if grad is not None:\n",
    "        \n",
    "        # reverse pass \n",
    "        _, (grad_W2, grad_b2, grad_h) = linear(W2,h,b2, grad)\n",
    "        _, dz = relu(z, grad_h)\n",
    "        _, (grad_W1, grad_b1, grad_x) = linear (W1,X,b1, dz)\n",
    "\n",
    "        grad_dict ={'W1':grad_W1.mean(axis=0),\n",
    "                    'b1':grad_b1.mean(axis=0),\n",
    "                    'W2':grad_W2.mean(axis=0),\n",
    "                    'b2':grad_b2.mean(axis=0)}       \n",
    "\n",
    "        return out.mean(), grad_dict\n",
    "    else:\n",
    "        return out.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0f3b8cd7-a2b1-4f27-bbce-ea738e1d9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict ={'W1':W1,'b1':b1,'W2':W2,'b2':b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ebb18297-1d3b-46db-9951-fee8f090b29d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.7242484222890089)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# forward mode\n",
    "out = myNN(X, param_dict)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "94dbc5e0-d713-4f6a-bf67-4dafce0b882c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(-1.7242484222890089)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reverse mode\n",
    "dfdf = np.ones((1,1))\n",
    "out, grad_dict = myNN(X, param_dict,np.ones((1,1)))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9001d687-79e4-41ec-9f69-7b609eb0a261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (16, 4)\n",
      "b1 (16,)\n",
      "W2 (1, 16)\n",
      "b2 (1,)\n"
     ]
    }
   ],
   "source": [
    "for k in param_dict.keys():\n",
    "    print(k,grad_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8690fe9-2597-4df2-ab74-81197e9d643e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a631ce-7123-41c8-be8f-00d201acb35b",
   "metadata": {},
   "source": [
    "## Step 4: Cross check the gradients in jax\n",
    "\n",
    "`Jax` is an **automatic differentiation** library where it can automatically keep track of the gradient propagation for us instead of us needing to keep track of everything manually with these \"forward\" and reverse modes.\n",
    "\n",
    "We'll cover another auto diff library, `pytorch` in more detail tomorrow, but we'll use jax here for x-checking the gradients as it's a very transparent interface for these types of checks.\n",
    "\n",
    "Also, it's super cute b/c the code stays _almost identical_, just replace `np` with `jnp`, so we can just reuse the forward pass NN code we wrote back in step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ce259cf1-fcc5-4a84-85e4-914a3da0e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import grad\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4adbdf9-a6c5-4a25-a896-3c47b30c66c2",
   "metadata": {},
   "source": [
    "Illustrative example: Calculate the gradient of :\n",
    "\n",
    "$$g(x) = x^2, \\qquad g'(x) = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4d4aa2ee-f2f7-4cf6-a03e-63a2ee910a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(xi):\n",
    "    return xi**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b1fc2082-c8a0-42ea-9c4a-8e3fc196c2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the gradient function\n",
    "grad_g = grad(g) # 2x\n",
    "\n",
    "# To evaluate the gradient, need to eval at a specific point, $x$\n",
    "grad_g(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a46fa584-082b-4adb-8470-35b158205b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the same parameters... just need to type cast to numpy\n",
    "param_dict_jax = {k:jnp.array(v) for k,v in param_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4eb3cc40-afca-4b55-abaa-6aa116afff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_jax(z):\n",
    "    return jnp.where(z>0,z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fb565620-2c7f-49ab-ac19-5eb2ae5e20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_jax(W,X,b):\n",
    "    '''\n",
    "    Linear layer mapping H = W x + b\n",
    "    - W: array (m,n)\n",
    "    - x: array (bs, n)\n",
    "    - b: array (m)\n",
    "\n",
    "    Outputs:\n",
    "    - H: array (bs,m)\n",
    "    '''\n",
    "    \n",
    "    return jnp.einsum('mn,bn->bm',W,X) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e93e469d-4e10-4a94-b28f-e45ceeaad333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X, param_dict)\n",
    "def myNN_jax(param_dict):\n",
    "\n",
    "    # Unpack the parameters\n",
    "    W1 = param_dict['W1']\n",
    "    b1 = param_dict['b1']\n",
    "    \n",
    "    W2 = param_dict['W2']\n",
    "    b2 = param_dict['b2']\n",
    "\n",
    "    # forward pass\n",
    "    z1 = linear_jax(W1, X, b1)\n",
    "    h1 = relu_jax(z1)\n",
    "\n",
    "    z2 = linear_jax(W2, h1, b2)\n",
    "    \n",
    "    return z2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e2a3138f-7645-4dcd-a8b9-1682e79e82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_nn = grad(myNN_jax)\n",
    "\n",
    "# And... evaluate it!\n",
    "grad_dict_jax = grad_nn(param_dict_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6dba56d4-1c27-4015-995d-686ed5d1f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (16, 4) (16, 4)\n",
      "W2 (1, 16) (1, 16)\n",
      "b1 (16,) (16,)\n",
      "b2 (1,) (1,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in grad_dict_jax.items():\n",
    "    print(k, v.shape, grad_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bef05af7-35c9-45ff-a5fa-55be0b29a117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 True\n",
      "W2 True\n",
      "b1 True\n",
      "b2 True\n"
     ]
    }
   ],
   "source": [
    "for k, v in grad_dict_jax.items():\n",
    "    print(k, np.all(np.isclose(v, grad_dict[k])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "695445a3-7179-41e6-9653-6eb1f8e83684",
   "metadata": {},
   "source": [
    "## Step 5: Connecting to the loss function\n",
    "\n",
    "<img src=\"../figures/hw3-Q5.jpg\" width=650/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ec0bf6-6c53-4d3c-9cd4-4d20a54d1c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8d331e-3d2b-4ba2-a35a-9f1b8a44b1c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7109d33f-1be3-4953-be5c-37b4e9cbf4a1",
   "metadata": {},
   "source": [
    "In theory, you can get gradients w/r.t. the full loss next, but this is where we'll stop the required homework, you've done a great job exercising your AD muscles!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30622afd-74b3-4469-a0e3-e3f4eb2b3e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dd303782-2064-4251-9b4d-7017543bc0ab",
   "metadata": {},
   "source": [
    "Awesome, you've played around with auto diff (AD), coded up both some intro examples and a simple NN, and cross checked these gradients with one of the autodiff packages on the market.\n",
    "\n",
    "For the rest of the course, we will take advantage of the AD and DL frameworks on the market (notably pytorch), but feel free to use any package you like here!!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
