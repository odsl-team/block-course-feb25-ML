{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30b64457",
   "metadata": {},
   "source": [
    "# Transformers and Arithmetic Addition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328a0f38",
   "metadata": {},
   "source": [
    "### Step 0: Some import and later required functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4ef79293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import mnist\n",
    "from matplotlib import pyplot as plt\n",
    "import matplotlib as mpl\n",
    "\n",
    "# Set a seed for reproducibility\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "os.environ['PYTHONHASHSEED'] = str(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b22ab",
   "metadata": {},
   "source": [
    "## Arithmetic Operation as Machine Translation Task\n",
    "\n",
    "In this exercise, you'll build a transformer model from scratch in PyTorch to perform simple arithmetic addition. Your model will learn to translate an input expression of two numbers (e.g., `\"123+456\"`) into its correct sum (e.g., `\"579\"`). This task is framed as a neural machine translation problem where the source is the arithmetic expression and the target is its result.\n",
    "\n",
    "\n",
    "### Dataset Overview\n",
    "This dataset is designed for a machine translation task where the goal is to perform addition between two numbers. Each sample consists of:\n",
    "- **Input (Source):** A string representing an arithmetic expression in the form \"a+b\", where *a* and *b* are randomly generated numbers.\n",
    "- **Output (Target):** A string representing the sum of *a* and *b*.\n",
    "\n",
    "Special tokens such as `<sos>` (start-of-sequence), `<eos>` (end-of-sequence), and `<pad>` (padding) are included in both the source and target vocabularies to facilitate sequence modeling in a Transformer-based neural machine translation framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e8e0ec10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming SimpleTokenizer is defined as provided:\n",
    "class SimpleTokenizer:\n",
    "    def __init__(self, texts=None):\n",
    "        self.char_to_idx = {}\n",
    "        self.idx_to_char = {}\n",
    "        \n",
    "        if texts:\n",
    "            self.fit(texts)\n",
    "    \n",
    "    def fit(self, texts):\n",
    "        # Create a set of all unique characters\n",
    "        unique_chars = set()\n",
    "        for text in texts:\n",
    "            unique_chars.update(text)\n",
    "        \n",
    "        # Create mapping dictionaries\n",
    "        self.char_to_idx = {char: idx for idx, char in enumerate(sorted(unique_chars))}\n",
    "        self.idx_to_char = {idx: char for char, idx in self.char_to_idx.items()}\n",
    "    \n",
    "    def encode(self, text):\n",
    "        return [self.char_to_idx[char] for char in text]\n",
    "    \n",
    "    def decode(self, indices):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices])\n",
    "    \n",
    "    @property\n",
    "    def vocab_size(self):\n",
    "        return len(self.char_to_idx)\n",
    "\n",
    "class AdditionDataset(Dataset):\n",
    "    def __init__(self, num_samples=10000, max_digits=6, add_special_tokens=True, tokenizer=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            num_samples (int): Number of samples to generate.\n",
    "            max_digits (int): Maximum number of digits for each number.\n",
    "            add_special_tokens (bool): Whether to add <sos> and <eos> to the sequences.\n",
    "            tokenizer (SimpleTokenizer, optional): A pre-initialized tokenizer. If None, it will be created.\n",
    "        \"\"\"\n",
    "        super(AdditionDataset, self).__init__()\n",
    "        self.num_samples = num_samples\n",
    "        self.max_digits = max_digits\n",
    "        self.add_special_tokens = add_special_tokens\n",
    "        self.samples = []\n",
    "        \n",
    "        # Generate the dataset samples.\n",
    "        for _ in range(num_samples):\n",
    "            a = random.randint(0, 10**max_digits - 1)\n",
    "            b = random.randint(0, 10**max_digits - 1)\n",
    "            src = f\"{a}+{b}\"\n",
    "            tgt = str(a + b)\n",
    "            if add_special_tokens:\n",
    "                src = \"<sos>\" + src + \"<eos>\"\n",
    "                tgt = \"<sos>\" + tgt + \"<eos>\"\n",
    "            self.samples.append((src, tgt))\n",
    "        \n",
    "        # If no tokenizer is provided, create one from all texts (both source and target)\n",
    "        if tokenizer is None:\n",
    "            all_texts = []\n",
    "            for src, tgt in self.samples:\n",
    "                all_texts.append(src)\n",
    "                all_texts.append(tgt)\n",
    "            self.tokenizer = SimpleTokenizer(all_texts)\n",
    "        else:\n",
    "            self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        src, tgt = self.samples[idx]\n",
    "        src_ids = self.tokenizer.encode(src)\n",
    "        tgt_ids = self.tokenizer.encode(tgt)\n",
    "        return torch.tensor(src_ids, dtype=torch.long), torch.tensor(tgt_ids, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9dc07bf",
   "metadata": {},
   "source": [
    "#### Step 0: Feel the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa807855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: <sos>229258+243962<eos> -> Target: <sos>473220<eos>\n",
      "Input: <sos>529903+631262<eos> -> Target: <sos>1161165<eos>\n",
      "Input: <sos>27824+588508<eos> -> Target: <sos>616332<eos>\n",
      "Input: <sos>208496+750800<eos> -> Target: <sos>959296<eos>\n",
      "Input: <sos>681453+735392<eos> -> Target: <sos>1416845<eos>\n"
     ]
    }
   ],
   "source": [
    "dataset = AdditionDataset(num_samples=5, max_digits=6)\n",
    "for i in range(len(dataset)):\n",
    "    src_tensor, tgt_tensor = dataset[i]\n",
    "    # Decode to verify using the dataset's tokenizer\n",
    "    src_decoded = dataset.tokenizer.decode(src_tensor.tolist())\n",
    "    tgt_decoded = dataset.tokenizer.decode(tgt_tensor.tolist())\n",
    "    print(f\"Input: {src_decoded} -> Target: {tgt_decoded}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7823988",
   "metadata": {},
   "source": [
    "# Building Transformers from scratch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9172689d",
   "metadata": {},
   "source": [
    "### Step 2: Multi-Head Attention Mechanism\n",
    "\n",
    "The multi-head attention mechanism is the \"secret sauce\" of transformers. Imagine reading a sentence where certain words carry more meaning than others. The attention mechanism allows the model to focus on the most relevant parts of the input sequence when generating each token of the output.\n",
    "\n",
    "The attention mechanism lets the model focus on relevant parts of the input. The \"multi-head\" aspect allows it to focus on different relationship patterns simultaneously. \"Multi-head\" means the model can capture different patterns simultaneously. For example, one head might capture subject-verb relationships while another focuses on adjective-noun connections. This diversity in attention is a key innovation that enhances the model's understanding and performance.\n",
    "\n",
    "#### Your Task:\n",
    "\n",
    "During the initialization of the module, make sure to:\n",
    "- Verify that `d_model` (the embedding dimension) is divisible by `num_heads`.\n",
    "- Compute the dimension per head (`d_k`), where `d_k = d_model / num_heads`.\n",
    "- Create linear layers for the query (Q), key (K), and value (V) projections.\n",
    "- Create a final linear layer for output projection.\n",
    "\n",
    "With the split_heads method reshape the input tensor to separate the embedding dimension into multiple heads. This allows each head to focus on different aspects of the input.\n",
    "\n",
    "In the forward pass:\n",
    "- Project the inputs using the linear layers and split into heads using the split_heads method.\n",
    "- Calculate attention scores by taking the dot product between queries and keys (transposed)\n",
    "- Scale the scores by dividing by sqrt(d_k) to prevent extremely small gradients\n",
    "- Apply a zero-mask if provided (important for preventing the model from \"seeing the future\" in autoregressive tasks), check `torch.Tensor.masked_fill`\n",
    "- Apply softmax to get normalized weights that sum to 1\n",
    "- Apply these weights to the values through matrix multiplication\n",
    "- Apply the output projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f8ed610e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert ..., \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = ...\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.w_q = ...\n",
    "        self.w_k = ...\n",
    "        self.w_v = ...\n",
    "        self.w_o = ...\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # Reshape from (batch_size, seq_len, d_model) to (batch_size, num_heads, seq_len, d_k)\n",
    "        x = x.view(batch_size, -1, self.num_heads, self.d_k)\n",
    "        return x.permute(0, 2, 1, 3)\n",
    "\n",
    "     def forward(self, q, k, v, mask=None):\n",
    "        batch_size = q.size(0)\n",
    "        \n",
    "        # Linear projections and split into heads\n",
    "        q = ...\n",
    "        k = ...\n",
    "        v = ...\n",
    "        \n",
    "        # Scaled dot-product attention\n",
    "        scores = ...\n",
    "        scores = ...\n",
    "\n",
    "        # Apply mask if provided\n",
    "        if mask is not None:\n",
    "            scores = ...\n",
    "        \n",
    "        # Apply softmax to get attention weights\n",
    "        attn_weights = ...\n",
    "        \n",
    "        # Apply attention weights to values\n",
    "        context = ...  # (batch_size, num_heads, seq_len_q, d_k)\n",
    "\n",
    "        # Reshape back to (batch_size, seq_len_q, d_model)\n",
    "        context = context.permute(0, 2, 1, 3).contiguous()\n",
    "        context = context.view(batch_size, -1, self.d_model)\n",
    "        \n",
    "        # Final linear projection\n",
    "        output = ...\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991c4e3",
   "metadata": {},
   "source": [
    "### Step 3: Position-wise Feed-Forward Network\n",
    "\n",
    "After the attention layer, each position in the sequence gets processed by this simple (feed-forward) neural network. It's applied to each position independently, like having the same mini neural network look at each word.\n",
    "\n",
    "#### Your Task:\n",
    "Write simple two-layer neural network with a ReLU activation in between.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52b1fcbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.fc1 = ...\n",
    "        self.fc2 = ...\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f17053d2",
   "metadata": {},
   "source": [
    "### Step 4: Positional Encoding\n",
    "\n",
    "Transformers don't inherently capture the order of tokens in a sequence. To address this, positional encoding is added to each token embedding to inject information about the token positions. This encoding is computed using sine and cosine functions of different frequencies, ensuring that each position in the sequence has a unique representation. The key trick is to use sine functions for even indices and cosine functions for odd indices in the embedding dimension.\n",
    "\n",
    "#### Your Task:\n",
    "Fill in the missing parts of the code below according to the steps\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7427c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_seq_length=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        \n",
    "        # TODO: Create a positional encoding matrix of size (max_seq_length, d_model) filled with zeros.\n",
    "        pe = ...\n",
    "        \n",
    "        # TODO: Create a tensor 'position' with values from 0 to max_seq_length - 1 and reshape it to (max_seq_length, 1).\n",
    "        position = ...\n",
    "        \n",
    "        # TODO: Compute the div_term using exponential decay based on the embedding dimension.\n",
    "        div_term = ...\n",
    "        \n",
    "        # TODO: For even indices (0, 2, ...), fill the positional encoding matrix with sine(position * div_term).\n",
    "        pe[:, 0::2] = ...\n",
    "        \n",
    "        # TODO: For odd indices (1, 3, ...), fill the positional encoding matrix with cosine(position * div_term).\n",
    "        pe[:, 1::2] = ...\n",
    "        \n",
    "        #Register the positional encoding matrix as a buffer so it's not treated as a model parameter.\n",
    "        self.register_buffer('pe', pe.unsqueeze(0))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # TODO: Add positional encoding to the input x. Make sure to slice pe based on the input's sequence length.\n",
    "        return ...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f8754c",
   "metadata": {},
   "source": [
    "### Step 5: Encoder Layer\n",
    "\n",
    "The encoder layer is the building block of the Transformer encoder. It combines several important components:\n",
    "\n",
    "- **Multi-Head Self-Attention Layer:**  \n",
    "  This layer computes attention where the query, key, and value all come from the same input. It allows the model to weigh the importance of different tokens in the sequence relative to each other. (use the provided `MultiHeadAttention` class).\n",
    "\n",
    "- **Feed-Forward Network:**  \n",
    "  A two-layer fully connected network that processes the output from the attention mechanism. (use the provided `PositionwiseFeedForward` class).\n",
    "\n",
    "- **Residual Connections:**  \n",
    "  These connections add the input of a sub-layer to its output, which helps the gradient flow during training. (See the last exercise)\n",
    "\n",
    "- **Layer Normalization:**  \n",
    "  Layer normalization is applied after adding the residual connection to stabilize and speed up training. See `nn.LayerNorm`\n",
    "\n",
    "- **Dropout Modules:**  \n",
    "  Dropout is used to prevent overfitting by randomly setting a fraction of the input units to zero during training. See `nn.Dropout`\n",
    "\n",
    "#### Your Task:\n",
    "Fill in the missing parts in the code below by completing the following steps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f8f5380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        \n",
    "        # TODO: Initialize the multi-head self-attention layer\n",
    "        self.self_attn = ...\n",
    "        \n",
    "        # TODO: Initialize the position-wise feed-forward network\n",
    "        self.feed_forward = ...\n",
    "        \n",
    "        # TODO: Initialize two layer normalization modules\n",
    "        self.norm1 = ...\n",
    "        self.norm2 = ...\n",
    "        \n",
    "        # TODO: Initialize two dropout modules\n",
    "        self.dropout1 = ...\n",
    "        self.dropout2 = ...\n",
    "\n",
    "    def forward(self, x, mask=None):\n",
    "        # TODO: Step 1: Apply multi-head self-attention with a residual connection and layer normalization.\n",
    "        # Q, K, and V are all the same input: x without mask for encoder\n",
    "        attn_output = ...  # Multi-head self-attention\n",
    "        # TODO: Add dropout and residual connection, then apply layer normalization.\n",
    "        x = ...\n",
    "        \n",
    "        # Step 2: Apply the feed-forward network with a residual connection and layer normalization.\n",
    "        ff_output = self.feed_forward(x)\n",
    "        # TODO: Add dropout and residual connection, then apply layer normalization.\n",
    "        x = ...\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87bbc75e",
   "metadata": {},
   "source": [
    "### Step 6: Decoder Layer\n",
    "\n",
    "The decoder layer extends the encoder layer by adding an extra cross-attention mechanism. In addition to self-attention (which uses a mask to prevent future token access), it incorporates cross-attention to allow the decoder to attend to the encoder's output. Like the encoder, the decoder layer uses residual connections and layer normalization to ensure effective gradient flow and stable learning.\n",
    "\n",
    "#### Your Task:\n",
    "Complete the following tasks by filling in the missing code sections:\n",
    "\n",
    "1. **Self-Attention:**  \n",
    "   Compute self-attention on the decoder input using a mask (`tgt_mask`) to prevent attending to future tokens. Then apply dropout, add a residual connection, and normalize.\n",
    "\n",
    "2. **Cross-Attention:**  \n",
    "   Use cross-attention where the queries come from the current decoder representation and the keys and values come from the encoder's output. Again, apply dropout, add the residual connection, and normalize.\n",
    "\n",
    "3. **Feed-Forward Network:**  \n",
    "   Process the result with a feed-forward network. Apply dropout, add the residual connection, and perform a final layer normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e01b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        \n",
    "        # TODO: Initialize the self-attention layer for the decoder.\n",
    "        self.self_attn = ...\n",
    "        \n",
    "        # TODO: Initialize the cross-attention layer for attending to encoder outputs.\n",
    "        self.cross_attn = ...\n",
    "        \n",
    "        # TODO: Initialize the position-wise feed-forward network.\n",
    "        self.feed_forward = ...\n",
    "        \n",
    "        # TODO: Initialize three layer normalization modules.\n",
    "        self.norm1 = ...\n",
    "        self.norm2 = ...\n",
    "        self.norm3 = ...\n",
    "        \n",
    "        # TODO: Initialize three dropout modules.\n",
    "        self.dropout1 = ...\n",
    "        self.dropout2 = ...\n",
    "        self.dropout3 = ...\n",
    "\n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        # TODO: Step 1: Self-attention on the decoder input with masking, tgt or src? ;)\n",
    "        self_attn_output = ...\n",
    "        # TODO: Apply dropout, add the residual connection, and normalize.\n",
    "        x = ...\n",
    "        \n",
    "        # TODO: Step 2: Cross-attention between decoder representation and encoder outputs with masking, tgt or src? ;)\n",
    "        cross_attn_output = ...\n",
    "        # TODO: Apply dropout, add the residual connection, and normalize.\n",
    "        x = ...\n",
    "        \n",
    "        # Step 3: Feed-forward network with residual connection and layer normalization.\n",
    "        ff_output = self.feed_forward(x)\n",
    "        # TODO: Apply dropout, add the residual connection, and normalize.\n",
    "        x = ...\n",
    "        \n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16a9b7c9",
   "metadata": {},
   "source": [
    "### Step 7: Full Encoder and Decoder\n",
    "\n",
    "Now we stack multiple encoder and decoder layers. The encoder simply applies multiple encoder layers in sequence. The decoder does the same with decoder layers, passing the encoder output to each layer. These stack multiple encoder or decoder layers to create deeper networks that can learn more complex patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba08841f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            EncoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, d_model, num_heads, d_ff, num_layers, dropout=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            DecoderLayer(d_model, num_heads, d_ff, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, enc_output, src_mask, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5bed6e",
   "metadata": {},
   "source": [
    "### Step 8: Complete Transformer Language Model\n",
    "\n",
    "In this step, you'll integrate all previous components to build a complete Transformer-based language model. This model can work in two different configurations: an encoder–decoder architecture or a decoder-only architecture. Understanding the differences between these two approaches is key to designing models for different NLP tasks.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Encoder–Decoder Architecture**\n",
    "\n",
    "- **When to Use:**  \n",
    "  This setup is ideal for tasks like machine translation where you have a clear input sequence (source language) and an output sequence (target language). The encoder processes the source sequence into a set of contextualized representations, and the decoder uses these representations to generate the target sequence.\n",
    "\n",
    "- **Pipeline Overview:**  \n",
    "  1. **Input Embedding:**  \n",
    "     Convert token IDs from the source sequence into dense vector representations. (Use `nn.Embedding`)\n",
    "  2. **Positional Encoding:**  \n",
    "     Add positional encodings to the embeddings so the model knows the order of tokens.\n",
    "  3. **Encoder Stack:**  \n",
    "     Pass the source embeddings through the encoder stack to generate a context-rich representation.\n",
    "  4. **Target Embedding & Positional Encoding:**  \n",
    "     Convert token IDs from the target sequence into embeddings ((Use `nn.Embedding`)) and add positional encodings.\n",
    "  5. **Decoder Stack:**  \n",
    "     Use the decoder stack to generate the target sequence. The decoder attends both to its own past predictions (self-attention with masking) and the encoder outputs (cross-attention).\n",
    "  6. **Output Projection:**  \n",
    "     Map the decoder's final outputs to vocabulary scores, which can then be used to predict the next token in the sequence.\n",
    "\n",
    "- **Hint for Implementation:**  \n",
    "  Make sure you correctly manage two inputs—the source and target sequences. The encoder output must be passed as an additional argument to the decoder so it can perform cross-attention.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Decoder-Only Architecture**\n",
    "\n",
    "- **When to Use:**  \n",
    "  This configuration is common in models like GPT for tasks such as language modeling or text generation. In this mode, the model uses only a decoder. The single input sequence is processed to predict the next token, relying on self-attention across all previous tokens.\n",
    "\n",
    "- **Pipeline Overview:**  \n",
    "  1. **Input Embedding:**  \n",
    "     Convert token IDs into vector representations (Use `nn.Embedding`).\n",
    "  2. **Positional Encoding:**  \n",
    "     Add positional information to the embeddings.\n",
    "  3. **Decoder Stack:**  \n",
    "     Process the sequence through a decoder stack that only employs self-attention (with a mask to prevent looking ahead).\n",
    "  4. **Output Projection:**  \n",
    "     Convert the processed vectors into vocabulary scores for next-token prediction.\n",
    "\n",
    "- **Hint for Implementation:**  \n",
    "  Since you only have one sequence, the same tensor is used both for the self-attention and as the input to subsequent layers. Ensure that you properly apply masking to prevent the decoder from \"cheating\" by looking at future tokens.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Components Across Both Architectures**\n",
    "\n",
    "1. **Embedding & Positional Encoding:**  \n",
    "   - **Embedding:** This layer converts each token (represented by its ID) into a high-dimensional vector.  \n",
    "   - **Scaling:** Multiplying by `sqrt(d_model)` helps balance the variance between embeddings and positional encodings.\n",
    "   - **Positional Encoding:** Adds order information, making sure that tokens' positions are known to the model.\n",
    "\n",
    "2. **Encoder and/or Decoder Stacks:**  \n",
    "   - **Encoder:** In the encoder–decoder model, the encoder transforms the input sequence into context-rich representations using self-attention and feed-forward networks.\n",
    "   - **Decoder:** Whether it's decoder-only or part of an encoder–decoder system, the decoder uses self-attention (and cross-attention in encoder–decoder) to generate output sequences.\n",
    "\n",
    "3. **Output Projection:**  \n",
    "   - This is typically a linear layer that maps the final hidden states from the decoder to logits over the vocabulary. These logits can be turned into probabilities to predict the next token.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Suggested Implementation Pipeline**\n",
    "\n",
    "1. **Start with the Embedding Layers:**  \n",
    "   - Write code to create the token embedding layer.\n",
    "   - Multiply the output by `sqrt(d_model)` for proper scaling.\n",
    "\n",
    "2. **Implement Positional Encoding:**  \n",
    "   - Create a module that adds positional encodings to the embeddings.\n",
    "   - Verify that the added positional information correctly alters the embeddings.\n",
    "\n",
    "3. **Build the Encoder and Decoder Stacks:**  \n",
    "   - For the encoder–decoder setup, create separate modules for the encoder and decoder.  \n",
    "   - For the decoder-only setup, focus on a robust decoder implementation that supports masking.\n",
    "\n",
    "4. **Integrate the Stacks with a Control Flag:**  \n",
    "   - Introduce a flag (e.g., `use_encoder_decoder`) in your model’s initialization.\n",
    "   - Use conditional logic in the forward pass to switch between encoder–decoder and decoder-only pipelines.\n",
    "\n",
    "5. **Add the Output Projection Layer:**  \n",
    "   - Implement a final linear layer that projects the decoder outputs to the size of your vocabulary.\n",
    "   - Test the end-to-end flow to ensure the model produces logits that can be used for training.\n",
    "\n",
    "6. **Test with Dummy Data:**  \n",
    "   - Before training, run a few forward passes with dummy data to verify that the dimensions and data flows are correct.\n",
    "   - Experiment with both modes (encoder–decoder and decoder-only) to understand the differences in output.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "500bc28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerLM(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, dropout=0.1, use_encoder_decoder=False):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size (int): Vocabulary size.\n",
    "            d_model (int): Embedding dimension.\n",
    "            num_heads (int): Number of attention heads.\n",
    "            d_ff (int): Dimension of the feed-forward network.\n",
    "            num_layers (int): Number of layers in the encoder/decoder stack.\n",
    "            dropout (float): Dropout rate.\n",
    "            use_encoder_decoder (bool): If True, use an encoder-decoder architecture; otherwise, use decoder-only.\n",
    "        \"\"\"\n",
    "        super(TransformerLM, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.use_encoder_decoder = use_encoder_decoder\n",
    "        \n",
    "        # TODO: Initialize token embedding layer\n",
    "        self.embedding = ...\n",
    "        # TODO: Initialize positional encoding module\n",
    "        self.positional_encoding = ...\n",
    "        self.dropout = ...\n",
    "        \n",
    "        if self.use_encoder_decoder:\n",
    "            # Initialize the encoder stack for the encoder-decoder architecture\n",
    "            self.encoder = Encoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "            # Initialize the decoder stack for the encoder-decoder architecture\n",
    "            self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        else:\n",
    "            # Initialize the decoder stack for the decoder-only architecture (like GPT)\n",
    "            self.decoder = Decoder(d_model, num_heads, d_ff, num_layers, dropout)\n",
    "        \n",
    "        # TODO: Initialize the output projection layer to map decoder outputs to vocabulary scores\n",
    "        self.fc_out = ...\n",
    "    \n",
    "    def forward(self, x, src_mask=None, tgt_mask=None, encoder_input=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x (Tensor): For decoder-only, the input sequence (batch, seq_len).\n",
    "                        For encoder-decoder, the target sequence (batch, tgt_seq_len).\n",
    "            src_mask (Tensor, optional): Mask for the encoder (if using encoder-decoder).\n",
    "            tgt_mask (Tensor, optional): Mask for the decoder.\n",
    "            encoder_input (Tensor, optional): Source sequence for the encoder (required if use_encoder_decoder=True).\n",
    "        \"\"\"\n",
    "        if self.use_encoder_decoder:\n",
    "            # --- Encoder–Decoder Setup ---\n",
    "            assert encoder_input is not None, \"encoder_input must be provided for encoder-decoder architecture.\"\n",
    "            \n",
    "            # TODO: Compute encoder embeddings: scale embeddings, add positional encoding, apply dropout.\n",
    "            enc_emb = ...\n",
    "            enc_emb = ...\n",
    "            enc_emb = ...\n",
    "            # TODO: Pass encoder embeddings through the encoder stack.\n",
    "            enc_output = ...\n",
    "            \n",
    "            # TODO: Compute decoder embeddings: scale embeddings, add positional encoding, apply dropout.\n",
    "            dec_emb = ...\n",
    "            dec_emb = ...\n",
    "            dec_emb = ...\n",
    "            # TODO: Pass decoder embeddings through the decoder stack, using encoder outputs.\n",
    "            dec_output = ...\n",
    "        else:\n",
    "            # --- Decoder-Only Setup ---\n",
    "            # TODO: Compute decoder embeddings: scale embeddings, add positional encoding, apply dropout.\n",
    "            dec_emb = ...\n",
    "            dec_emb = ...\n",
    "            dec_emb = ...\n",
    "            # TODO: Pass decoder embeddings through the decoder stack (using self-attention only).\n",
    "            dec_output = ...\n",
    "        \n",
    "        # Project the decoder output to vocabulary scores using the output projection layer.\n",
    "        output = self.fc_out(dec_output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135b40b2",
   "metadata": {},
   "source": [
    "### Step 9: Causal Masking for Autoregressive Generation\n",
    "In language modeling, we need to ensure the model only looks at past tokens when predicting the next one, by creating a triangular mask that allows each position to attend only to previous positions and itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a6cbe7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(seq_len):\n",
    "    # TODO: Create a causal attention mask to prevent attending to future tokens.\n",
    "    mask = ...\n",
    "    return mask == 0  # Convert to boolean mask where True values are kept"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf19bd63",
   "metadata": {},
   "source": [
    "### Step 10: Training and Inference Function\n",
    "\n",
    "Now i the follwoing, you have both training and inference pipelines, it's time to explore and compare the performance of the decoder-only and encoder–decoder architectures. In this step, you will:\n",
    "\n",
    "1. **Plot the Training Loss:**  \n",
    "   Track and plot the training loss over epochs to understand how well each model converges during training.\n",
    "\n",
    "2. **Evaluate on a Length Generalization Task:**  \n",
    "   In a length generalization task, you test the model on arithmetic expressions that are longer than those seen during training. For example, if you trained your model with numbers up to 6 digits, you could test it on arithmetic expressions with 7 or 8 digits. The goal is to assess whether the model can extrapolate its learned arithmetic operations to longer sequences. Report the performance of the models within this task.\n",
    "   \n",
    "   **What is Length Generalization?**  \n",
    "   Length generalization examines a model's ability to handle sequences longer than those encountered during training. In our arithmetic task, it tests whether the model, trained on relatively short input sequences, can correctly perform addition when the numbers have more digits than the training examples. This is a key challenge in many sequence-to-sequence tasks and is an active area of research.\n",
    "\n",
    "3. **Research on Solutions for Length Generalization:**  \n",
    "   As you analyze the results, do some research on strategies that have been proposed to improve length generalization. For instance, look into:\n",
    "   - Modifications in positional encoding\n",
    "   - Modifications on non-linear activations \n",
    "   - Architectures that incorporate recurrence or other forms of extrapolation-friendly inductive biases\n",
    "   - Techniques like curriculum learning, where the model is gradually exposed to longer sequences\n",
    "\n",
    "#### Your Tasks:\n",
    "- **Task 1:** Modify your training loop to record the loss after every epoch (or batch) and plot these loss curves for both decoder-only and encoder–decoder models.\n",
    "- **Task 2:** Create a test set with arithmetic expressions involving longer numbers (e.g., 7- or 8-digit numbers) and evaluate the performance of your trained models on this set.\n",
    "- **Task 3:** Document your observations. How do the two architectures compare in terms of training loss and performance on the length generalization task?\n",
    "- **Task 4:** Perform a brief literature search (or research online) to identify at least two different strategies that have been proposed to enhance length generalization. Summarize your findings.\n",
    "\n",
    "#### Hints for Implementation:\n",
    "- **Creating a Length Generalization Test Set:**  \n",
    "  Modify your data generation function (or create a new dataset) where `max_digits` is increased (e.g., 7 or 8) to simulate the extrapolation scenario.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0e051d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_transformer_lm(model, dataloader, num_epochs, learning_rate, device):\n",
    "    \"\"\"\n",
    "    Trains the transformer language model.\n",
    "    \n",
    "    For encoder–decoder mode:\n",
    "        - Each batch returns (src, tgt).\n",
    "        - We create a causal mask for the target.\n",
    "        - The model is called with: model(tgt, src_mask, tgt_mask, encoder_input=src)\n",
    "        \n",
    "    For decoder-only mode:\n",
    "        - We combine src and tgt into a single sequence.\n",
    "        - We create a causal mask for the combined sequence.\n",
    "        - The model is called with: model(combined, causal_mask)\n",
    "    \"\"\"\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        \n",
    "        for batch_idx, (src, tgt) in enumerate(dataloader):\n",
    "            # Move data to device\n",
    "            src, tgt = src.to(device), tgt.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if model.use_encoder_decoder:\n",
    "                # --- Encoder–Decoder Mode ---\n",
    "                # (For arithmetic translation: src = \"123+456\", tgt = \"579\")\n",
    "                # Create target causal mask to prevent looking ahead\n",
    "                tgt_mask = create_causal_mask(tgt.size(1)).to(device)\n",
    "                # For this task, we do not apply a source mask (or you can create one if needed)\n",
    "                src_mask = None\n",
    "                # Forward pass: decoder receives tgt and encoder gets src.\n",
    "                outputs = model(tgt, src_mask, tgt_mask, encoder_input=src)\n",
    "                # In teacher forcing, we compute loss against the target sequence.\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), tgt.view(-1))\n",
    "            else:\n",
    "                # --- Decoder-Only Mode ---\n",
    "                # For a translation-style task using a decoder-only model,\n",
    "                # we combine the source and target into one sequence.\n",
    "                # For instance, if src = \"<sos>123+456<eos>\" and tgt = \"<sos>579<eos>\",\n",
    "                # we combine as: combined = src + tgt[1:] to avoid duplicating <sos>.\n",
    "                combined = torch.cat([src, tgt[:, 1:]], dim=1)\n",
    "                seq_len = combined.size(1)\n",
    "                causal_mask = create_causal_mask(seq_len).to(device)\n",
    "                outputs = model(combined, causal_mask)\n",
    "                # The target is the combined sequence shifted by one.\n",
    "                loss = criterion(outputs.view(-1, outputs.size(-1)), combined.view(-1))\n",
    "            \n",
    "            # Backpropagation and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            if (batch_idx + 1) % 10 == 0:\n",
    "                print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx+1}/{len(dataloader)}, '\n",
    "                      f'Loss: {total_loss / (batch_idx + 1):.4f}')\n",
    "        \n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Average Loss: {total_loss / len(dataloader):.4f}')\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "27774218",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(model, tokenizer, start_text, max_length, temperature=1.0, device='cpu', src_text=None):\n",
    "    \"\"\"\n",
    "    Generates text from the transformer model.\n",
    "    \n",
    "    For encoder–decoder mode:\n",
    "        - src_text is the input arithmetic expression (e.g. \"123+456\" with special tokens).\n",
    "        - start_text is the initial target prompt (e.g. \"<sos>\").\n",
    "    For decoder-only mode:\n",
    "        - start_text is used as the prompt.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    if model.use_encoder_decoder:\n",
    "        # --- Encoder–Decoder Mode ---\n",
    "        # Ensure a source text is provided.\n",
    "        assert src_text is not None, \"For encoder-decoder mode, please provide src_text.\"\n",
    "        \n",
    "        # Encode the source text and move to device.\n",
    "        encoder_input = tokenizer.encode(src_text)\n",
    "        encoder_input = torch.tensor([encoder_input], dtype=torch.long).to(device)\n",
    "        \n",
    "        # Encode the starting target text.\n",
    "        input_seq = tokenizer.encode(start_text)\n",
    "        input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
    "        \n",
    "        for _ in range(max_length):\n",
    "            tgt_mask = create_causal_mask(input_tensor.size(1)).to(device)\n",
    "            # Forward pass with encoder input.\n",
    "            outputs = model(input_tensor, None, tgt_mask, encoder_input=encoder_input)\n",
    "            next_token_logits = outputs[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor, \n",
    "                torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "            ], dim=1)\n",
    "            if next_token == tokenizer.char_to_idx.get(\"<eos>\", -1):\n",
    "                break\n",
    "        \n",
    "        generated_tokens = input_tensor[0].tolist()\n",
    "        generated_text = tokenizer.decode(generated_tokens)\n",
    "        return generated_text\n",
    "    \n",
    "    else:\n",
    "        # --- Decoder-Only Mode ---\n",
    "        input_seq = tokenizer.encode(start_text)\n",
    "        input_tensor = torch.tensor([input_seq], dtype=torch.long).to(device)\n",
    "        for _ in range(max_length):\n",
    "            seq_len = input_tensor.size(1)\n",
    "            causal_mask = create_causal_mask(seq_len).to(device)\n",
    "            outputs = model(input_tensor, causal_mask)\n",
    "            next_token_logits = outputs[0, -1, :] / temperature\n",
    "            probabilities = F.softmax(next_token_logits, dim=-1)\n",
    "            next_token = torch.multinomial(probabilities, 1).item()\n",
    "            input_tensor = torch.cat([\n",
    "                input_tensor,\n",
    "                torch.tensor([[next_token]], dtype=torch.long).to(device)\n",
    "            ], dim=1)\n",
    "            if next_token == tokenizer.char_to_idx.get(\"<eos>\", -1):\n",
    "                break\n",
    "        \n",
    "        generated_tokens = input_tensor[0].tolist()\n",
    "        generated_text = tokenizer.decode(generated_tokens)\n",
    "        return generated_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c95e5120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 16\n",
    "d_model = 16\n",
    "num_heads = 4\n",
    "d_ff = 16\n",
    "num_layers = 2\n",
    "dropout = 0.1\n",
    "num_epochs = 10\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c8b30be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Epoch 1/10, Batch 10/625, Loss: 2.6297\n",
      "Epoch 1/10, Batch 20/625, Loss: 2.4006\n",
      "Epoch 1/10, Batch 30/625, Loss: 2.2132\n",
      "Epoch 1/10, Batch 40/625, Loss: 2.0554\n",
      "Epoch 1/10, Batch 50/625, Loss: 1.9205\n",
      "Epoch 1/10, Batch 60/625, Loss: 1.8045\n",
      "Epoch 1/10, Batch 70/625, Loss: 1.7010\n",
      "Epoch 1/10, Batch 80/625, Loss: 1.6097\n",
      "Epoch 1/10, Batch 90/625, Loss: 1.5264\n",
      "Epoch 1/10, Batch 100/625, Loss: 1.4504\n",
      "Epoch 1/10, Batch 110/625, Loss: 1.3808\n",
      "Epoch 1/10, Batch 120/625, Loss: 1.3173\n",
      "Epoch 1/10, Batch 130/625, Loss: 1.2577\n",
      "Epoch 1/10, Batch 140/625, Loss: 1.2025\n",
      "Epoch 1/10, Batch 150/625, Loss: 1.1512\n",
      "Epoch 1/10, Batch 160/625, Loss: 1.1036\n",
      "Epoch 1/10, Batch 170/625, Loss: 1.0592\n",
      "Epoch 1/10, Batch 180/625, Loss: 1.0177\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 47\u001b[0m\n\u001b[1;32m     32\u001b[0m model \u001b[38;5;241m=\u001b[39m TransformerLM(\n\u001b[1;32m     33\u001b[0m     vocab_size\u001b[38;5;241m=\u001b[39mvocab_size,\n\u001b[1;32m     34\u001b[0m     d_model\u001b[38;5;241m=\u001b[39md_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     39\u001b[0m     use_encoder_decoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# Change this flag to switch architectures\u001b[39;00m\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Print model summary and parameter count\u001b[39;00m\n\u001b[1;32m     43\u001b[0m \u001b[38;5;66;03m#print(model)\u001b[39;00m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m#print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 47\u001b[0m model \u001b[38;5;241m=\u001b[39m train_transformer_lm(model, dataloader, num_epochs, learning_rate, device)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# Generate text:\u001b[39;00m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;66;03m# For encoder-decoder mode, we need a source text (the arithmetic expression) and a starting target prompt.\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;66;03m# For example, src_text is \"<sos>123+456<eos>\" and start_text is \"<sos>\"\u001b[39;00m\n\u001b[1;32m     52\u001b[0m start_txt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<sos>\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "Cell \u001b[0;32mIn[18], line 55\u001b[0m, in \u001b[0;36mtrain_transformer_lm\u001b[0;34m(model, dataloader, num_epochs, learning_rate, device)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[38;5;66;03m# Backpropagation and optimization\u001b[39;00m\n\u001b[1;32m     54\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 55\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     57\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (batch_idx \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:391\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    387\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    388\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    389\u001b[0m             )\n\u001b[0;32m--> 391\u001b[0m out \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    392\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    394\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/optimizer.py:76\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[1;32m     75\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 76\u001b[0m     ret \u001b[38;5;241m=\u001b[39m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     78\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:168\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    157\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    159\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    160\u001b[0m         group,\n\u001b[1;32m    161\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    165\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    166\u001b[0m         state_steps)\n\u001b[0;32m--> 168\u001b[0m     adam(\n\u001b[1;32m    169\u001b[0m         params_with_grad,\n\u001b[1;32m    170\u001b[0m         grads,\n\u001b[1;32m    171\u001b[0m         exp_avgs,\n\u001b[1;32m    172\u001b[0m         exp_avg_sqs,\n\u001b[1;32m    173\u001b[0m         max_exp_avg_sqs,\n\u001b[1;32m    174\u001b[0m         state_steps,\n\u001b[1;32m    175\u001b[0m         amsgrad\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mamsgrad\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    176\u001b[0m         has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    177\u001b[0m         beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    178\u001b[0m         beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    179\u001b[0m         lr\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlr\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    180\u001b[0m         weight_decay\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    181\u001b[0m         eps\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124meps\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    182\u001b[0m         maximize\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaximize\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    183\u001b[0m         foreach\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mforeach\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    184\u001b[0m         capturable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcapturable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    185\u001b[0m         differentiable\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    186\u001b[0m         fused\u001b[38;5;241m=\u001b[39mgroup[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfused\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m    187\u001b[0m         grad_scale\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgrad_scale\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    188\u001b[0m         found_inf\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound_inf\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[1;32m    189\u001b[0m     )\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:318\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    316\u001b[0m     func \u001b[38;5;241m=\u001b[39m _single_tensor_adam\n\u001b[0;32m--> 318\u001b[0m func(params,\n\u001b[1;32m    319\u001b[0m      grads,\n\u001b[1;32m    320\u001b[0m      exp_avgs,\n\u001b[1;32m    321\u001b[0m      exp_avg_sqs,\n\u001b[1;32m    322\u001b[0m      max_exp_avg_sqs,\n\u001b[1;32m    323\u001b[0m      state_steps,\n\u001b[1;32m    324\u001b[0m      amsgrad\u001b[38;5;241m=\u001b[39mamsgrad,\n\u001b[1;32m    325\u001b[0m      has_complex\u001b[38;5;241m=\u001b[39mhas_complex,\n\u001b[1;32m    326\u001b[0m      beta1\u001b[38;5;241m=\u001b[39mbeta1,\n\u001b[1;32m    327\u001b[0m      beta2\u001b[38;5;241m=\u001b[39mbeta2,\n\u001b[1;32m    328\u001b[0m      lr\u001b[38;5;241m=\u001b[39mlr,\n\u001b[1;32m    329\u001b[0m      weight_decay\u001b[38;5;241m=\u001b[39mweight_decay,\n\u001b[1;32m    330\u001b[0m      eps\u001b[38;5;241m=\u001b[39meps,\n\u001b[1;32m    331\u001b[0m      maximize\u001b[38;5;241m=\u001b[39mmaximize,\n\u001b[1;32m    332\u001b[0m      capturable\u001b[38;5;241m=\u001b[39mcapturable,\n\u001b[1;32m    333\u001b[0m      differentiable\u001b[38;5;241m=\u001b[39mdifferentiable,\n\u001b[1;32m    334\u001b[0m      grad_scale\u001b[38;5;241m=\u001b[39mgrad_scale,\n\u001b[1;32m    335\u001b[0m      found_inf\u001b[38;5;241m=\u001b[39mfound_inf)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/optim/adam.py:394\u001b[0m, in \u001b[0;36m_single_tensor_adam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, grad_scale, found_inf, amsgrad, has_complex, beta1, beta2, lr, weight_decay, eps, maximize, capturable, differentiable)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;66;03m# Decay the first and second moment running average coefficient\u001b[39;00m\n\u001b[1;32m    393\u001b[0m exp_avg\u001b[38;5;241m.\u001b[39mlerp_(grad, \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1)\n\u001b[0;32m--> 394\u001b[0m exp_avg_sq\u001b[38;5;241m.\u001b[39mmul_(beta2)\u001b[38;5;241m.\u001b[39maddcmul_(grad, grad\u001b[38;5;241m.\u001b[39mconj(), value\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta2)\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m capturable \u001b[38;5;129;01mor\u001b[39;00m differentiable:\n\u001b[1;32m    397\u001b[0m     step \u001b[38;5;241m=\u001b[39m step_t\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Determine device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Initialize our arithmetic dataset\n",
    "# Our dataset returns (src, tgt) pairs with special tokens\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(batch):\n",
    "    # batch is a list of tuples (src_tensor, tgt_tensor)\n",
    "    src_seqs, tgt_seqs = zip(*batch)\n",
    "    \n",
    "    # Determine the padding token index from the tokenizer. Here we assume <pad> is present.\n",
    "    pad_idx = dataset.tokenizer.char_to_idx.get(\"<pad>\", 0)\n",
    "    \n",
    "    # Pad the sequences\n",
    "    src_padded = pad_sequence(src_seqs, batch_first=True, padding_value=pad_idx)\n",
    "    tgt_padded = pad_sequence(tgt_seqs, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    return src_padded, tgt_padded\n",
    "\n",
    "dataset = AdditionDataset(num_samples=10000, max_digits=6, add_special_tokens=True)\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_fn)\n",
    "\n",
    "# Set up vocabulary size from the dataset's tokenizer\n",
    "# (Our modified AdditionDataset creates a tokenizer based on the data if none is provided)\n",
    "vocab_size = dataset.tokenizer.vocab_size if hasattr(dataset, 'tokenizer') else len(dataset.src_vocab)\n",
    "\n",
    "# Initialize the model\n",
    "# Set use_encoder_decoder=True for encoder-decoder mode or False for decoder-only mode.\n",
    "model = TransformerLM(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    num_heads=num_heads,\n",
    "    d_ff=d_ff,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    use_encoder_decoder=True  # Change this flag to switch architectures\n",
    ")\n",
    "\n",
    "# Print model summary and parameter count\n",
    "print(model)\n",
    "print(f\"Number of parameters: {sum(p.numel() for p in model.parameters())}\")\n",
    "\n",
    "# Train the model\n",
    "model = train_transformer_lm(model, dataloader, num_epochs, learning_rate, device)\n",
    "\n",
    "# Generate text:\n",
    "# For encoder-decoder mode, we need a source text (the arithmetic expression) and a starting target prompt.\n",
    "# For example, src_text is \"<sos>123+456<eos>\" and start_text is \"<sos>\"\n",
    "start_txt = \"<sos>\"\n",
    "src_text = \"<sos>123+456<eos>\"  # Change this expression to test different arithmetic problems\n",
    "\n",
    "generated_text = generate_text(model, dataset.tokenizer, start_txt, max_length=30, temperature=1.0, device=device, src_text=src_text)\n",
    "print(f\"Generated text:\\n{generated_text}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9314e838",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
