{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "288a23c5-4388-45b4-94ab-f0af2088d824",
   "metadata": {},
   "source": [
    "# Homework 3: Auto diff in numpy\n",
    "\n",
    "Iterations on a classical theme...\n",
    "\n",
    "In tutorials we've coded up networks in pytorch, julia... and this is one last exercise!\n",
    "However, since we have coded up custom pullbacks in julia already, if you'd like to answer the questions in this notebook by submitting a simple julia script instead, we'll also accept that solution!\n",
    "\n",
    "The goal of this exercise is just to cross check your work from pencil and paper math, to what you code up yourself, to some AD package :)\n",
    "\n",
    "**Table of Contents**\n",
    "1. Computational graphs \n",
    "2. Build a NN\n",
    "3. Calc NN gradients\n",
    "4. Check gradients in pytorch\n",
    "5. Add a loss ✏️ \n",
    "\n",
    "Nicole Hartman\n",
    "\n",
    "ODSL ML Block Course \n",
    "\n",
    "Originally from Apr 2024, last revised Feb 2025\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ab3ec40-2f2c-4e71-b96c-c5b6380e076f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a5a566-c2bf-4d72-a60c-98b4a33d6883",
   "metadata": {},
   "source": [
    "## Step 1: Computational graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e1e4d5-6731-4583-b71a-6cdd3ae3d227",
   "metadata": {},
   "source": [
    "**Motivating example:**  Let's code up the model we had worked out in lecture.\n",
    "\n",
    "$$f(x,y,z) = (x+y) \\cdot z$$\n",
    "\n",
    "<span style=\"color:red\">$$f_1(x,y) = q = x+y$$</span>\n",
    "\n",
    "<span style=\"color:blue\">$$f_2(q,z) = q \\cdot z$$</span>\n",
    "\n",
    "<img src=\"../figures/toy-ex.jpg\" width=650/>\n",
    "\n",
    "**To Do:** Implement `f1`, `f2` and `f` functions in numpy with both a forward and a backward mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "bf1022aa-71a4-4ab1-a4fd-9b5f542549d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(x,y, grad=None):\n",
    "    '''\n",
    "    f1(x,y) = x+y\n",
    "    \n",
    "    Inputs:\n",
    "    - x,y: The inputs to the functoin\n",
    "    - grad: if not None, this is the \"upstream gradient\"\n",
    "\n",
    "    Outpus:\n",
    "    - If grad is passed, return f(x,y), grad\n",
    "    - Else, just the \"normal\" fct, return f1(x,y)\n",
    "    '''\n",
    "    \n",
    "    out = \n",
    "    if grad:\n",
    "        raise NotImplementedError\n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "c0e4b3e8-116e-4727-81d8-69e2843d35a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f2(q,z, grad=None):\n",
    "    '''\n",
    "    f2(q,z) = q*z\n",
    "    '''\n",
    "\n",
    "    out =\n",
    "    if grad:\n",
    "       raise NotImplementedError\n",
    "    else: \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "5941b7b7-de97-49f8-992e-672758648f3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x,y,z, grad=None):\n",
    "    '''\n",
    "    f(x,y,z) = f2(q,x)\n",
    "    '''\n",
    "\n",
    "    # Need to run the program 1x to get the values to eval the grad\n",
    "    q = \n",
    "    out = \n",
    "\n",
    "    if grad:\n",
    "\n",
    "        '''\n",
    "        Fill in\n",
    "        '''\n",
    "        \n",
    "        return out, [dfdx, dfdy, dfdz]\n",
    "    \n",
    "    else:\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2b1e2a2-a2fe-4a67-8997-1052be88420e",
   "metadata": {},
   "source": [
    "Check the forward mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "fce6e1be-3907-4442-b162-33a1e56f5e74",
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y,z = 5,-2,4\n",
    "print(f(x,y,z))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf31f60-adf1-4e51-bea5-087feaafc3dc",
   "metadata": {},
   "source": [
    "\n",
    "**What about the derivative?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "f12d235d-33a6-40a6-9936-f28a361bb8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfdf = 1\n",
    "out, grad = f(x,y,z, grad=dfdf)\n",
    "\n",
    "for xi, dfi in zip(['x','y','z'],grad):\n",
    "    print(f'df/d{xi}= {dfi}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d4cf94-1812-4b1e-bdce-8c96b0f9c021",
   "metadata": {},
   "source": [
    "## Step 2: Multi-layer perceptron (forward pass)\n",
    "\n",
    "Define a simple feed-forward NN that accepts a input matrix\n",
    "\n",
    "$$X \\in \\mathbb{R}^{N \\times d}$$\n",
    "\n",
    "(where for the example just above $N=200$, $d=5$)\n",
    "\n",
    "and constructs an MLP with a single hidden layer with $H = 16$ units, and a ReLU non-linearity.\n",
    "\n",
    "**Hint:** this is just three equations:\n",
    "\n",
    "$$z = X W^T + b_1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648c20e1-2788-41e3-bb41-555cc7931d22",
   "metadata": {},
   "source": [
    "$$h = \\mathrm{ReLU}(z_1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e6cdfbf-0eea-4034-95cf-2cc45e6518a1",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "$$y_\\textrm{pred} =h W_2^T  + b_2$$\n",
    "\n",
    "where $W_1 \\in \\mathbb{R}^{H \\times d}, b_1 \\in \\mathbb{R}^{H}, W_2 \\in \\mathbb{R}^{1 \\times H}, b_2 \\in \\mathbb{R}$,\n",
    "$z \\in \\mathbb{R}^{N\\times H}$, $h \\in \\mathbb{R}^{N \\times H}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4fdd7f-9696-4240-bdcc-5d3286eadf3e",
   "metadata": {},
   "source": [
    "**Potentially useful functions:** \n",
    "- `np.einsum`\n",
    "- `np.where`\n",
    "- `np.random.randn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cffc2a91-9055-45f2-a6a5-f2f54578be4d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m n \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[1;32m      3\u001b[0m d \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m----> 4\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(n,d)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "# We'll just use a random dataset for this problem\n",
    "n = 128\n",
    "d = 4\n",
    "X = np.random.randn(n,d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "1dd4b75b-c2a8-48e2-ab9f-c1b460ccf765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(16, 5)\n",
      "(16,)\n",
      "(1, 16)\n",
      "(1,)\n"
     ]
    }
   ],
   "source": [
    "# Initialize the parameters (randomly) to have the desired shape\n",
    "H = 16\n",
    "\n",
    "'''\n",
    "TO DO: Initialize the parameters\n",
    "'''\n",
    "\n",
    "# 1st layer of the NN\n",
    "W1 = np.random.randn( # TO DO : fill in\n",
    "b1 = np.random.randn( # TO DO : fill in\n",
    "\n",
    "# Now the 2nd layer of the NN\n",
    "W2 = np.random.randn( # TO DO : fill in\n",
    "b2 = np.random.randn( # TO DO : fill in\n",
    "\n",
    "for v in [W1,b1,W2,b2]:\n",
    "    print(v.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434e1654-ec6c-432f-9b42-20e6cf69974a",
   "metadata": {},
   "source": [
    "Do the output shapes make sense?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab05ccd-f9af-4df1-b66a-cabcd937c219",
   "metadata": {},
   "source": [
    "$$ReLU(x) = \\max(x,0)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "0b5e09c8-11f3-4898-bf71-efa74a168335",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return np.where(x>0, x, 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "9f91caa6-1558-4cda-bf1b-b6660d98f13b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 3, 0])"
      ]
     },
     "execution_count": 203,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.array([1,3,-1])\n",
    "relu(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "0141bf7a-7df2-4c01-aa04-2c07baf64be9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 16)\n",
      "(200, 16)\n",
      "(200, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set up the feedforward model\n",
    "\n",
    "# z = \"W1x + b1\"\n",
    "z = np.einsum('hj,ij->ih',W1,X) + b1\n",
    "print(z.shape)\n",
    "\n",
    "h = relu(z)\n",
    "print(h.shape)\n",
    "y_pred = np.einsum('oh,ih->io',W2,h) + b2\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40a3e3-39c1-4b92-bd02-ad7c3bb5b1c5",
   "metadata": {},
   "source": [
    "We're thinking about a regression task right now, so no need to have a non-linearity on the last layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "be087011-6d57-496b-8fa5-9f68ba3c7f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check: Does the output have the dimensionality we expect?\n",
    "assert y_pred.shape[0] == N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3549f9ce-4878-43ca-be2f-00cfe25597df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73adc084-eb39-4831-a7d5-2d36b60002f2",
   "metadata": {},
   "source": [
    "## Step 3: Gradients of a NN\n",
    "\n",
    "OK, let's re-code our \"neural network\" up, but this time with the gradient computation so we can do _back prop_. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "d6ea1e0c-4b15-4715-b0d4-fdc9be7d8995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear(W,X,b, grad=None):\n",
    "    '''\n",
    "    Linear layer mapping H = W x + b\n",
    "    - W: array (m,n)\n",
    "    - x: array (bs, n)\n",
    "    - b: array (m)\n",
    "\n",
    "    Outputs:\n",
    "    - H: array (bs,m)\n",
    "\n",
    "    If grad is true, return tuple: out, (grad_W, grad_b, grad_H)\n",
    "    - grad_W: array (bs, m, n)\n",
    "    - grad_b: array (bs, m)\n",
    "    - grad_X: array (bs,    n)\n",
    "    '''\n",
    "\n",
    "    assert X.shape[1] == W.shape[1]\n",
    "    assert W.shape[0] == b.shape[0]\n",
    "\n",
    "    '''\n",
    "    TO DO: Fill in\n",
    "    '''\n",
    "    raise NotImplementedError\n",
    "    \n",
    "    H = \n",
    "    \n",
    "    if grad is not None:\n",
    "        \n",
    "        grad_W = \n",
    "        grad_b = \n",
    "        grad_X = \n",
    "        \n",
    "        return H, (grad_W, grad_b, grad_X)\n",
    "    else:\n",
    "        return H"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "849c6d4b-25b8-45f0-9d63-4ea1ac07f1fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(z,grad=None):\n",
    "    '''\n",
    "    Non-linearity: better gradient flow\n",
    "    '''\n",
    "    out = \n",
    "\n",
    "    if grad is not None:\n",
    "        '''\n",
    "        To do: Calculate the local grad and return it too\n",
    "        ''' \n",
    "        raise NotImplementedError \n",
    "    else:\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2ce982f0-1f53-433e-bbd2-9c73a88786c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def myNN(X, param_dict, grad=None):\n",
    "    '''\n",
    "    Build an MLP with a single hidden layer.\n",
    "    '''\n",
    "\n",
    "    # Unpack the parameter dict\n",
    "    W1, b1 = param_dict[\"W1\"], param_dict[\"b1\"]\n",
    "    W2, b2 = param_dict[\"W2\"], param_dict[\"b2\"]\n",
    "\n",
    "    # forward pass\n",
    "    '''\n",
    "    TO DO: Fill in\n",
    "    '''\n",
    "\n",
    "    if grad is not None:\n",
    "        \n",
    "        # reverse pass \n",
    "        '''\n",
    "        TO DO: Fill in\n",
    "        '''\n",
    "        \n",
    "\n",
    "        grad_dict ={'W1':grad_W1.mean(axis=0),\n",
    "                    'b1':grad_b1.mean(axis=0),\n",
    "                    'W2':grad_W2.mean(axis=0),\n",
    "                    'b2':grad_b2.mean(axis=0)}       \n",
    "\n",
    "        return out.mean(), grad_dict\n",
    "    else:\n",
    "        return out.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0f3b8cd7-a2b1-4f27-bbce-ea738e1d9dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "param_dict ={'W1':W1,'b1':b1,'W2':W2,'b2':b2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "ebb18297-1d3b-46db-9951-fee8f090b29d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward mode\n",
    "out = myNN(X, param_dict)\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "94dbc5e0-d713-4f6a-bf67-4dafce0b882c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reverse mode\n",
    "dfdf = np.ones((1,1))\n",
    "out, grad_dict = myNN(X, param_dict,np.ones((1,1)))\n",
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9001d687-79e4-41ec-9f69-7b609eb0a261",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in param_dict.keys():\n",
    "    print(k,grad_dict[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8690fe9-2597-4df2-ab74-81197e9d643e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15a631ce-7123-41c8-be8f-00d201acb35b",
   "metadata": {},
   "source": [
    "## Step 4: Cross check the gradients in jax\n",
    "\n",
    "`Jax` is an **automatic differentiation** library where it can automatically keep track of the gradient propagation for us instead of us needing to keep track of everything manually with these \"forward\" and reverse modes.\n",
    "\n",
    "We'll cover another auto diff library, `pytorch` in more detail tomorrow, but we'll use jax here for x-checking the gradients as it's a very transparent interface for these types of checks.\n",
    "\n",
    "Also, it's super cute b/c the code stays _almost identical_, just replace `np` with `jnp`, so we can just reuse the forward pass NN code we wrote back in step 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "ce259cf1-fcc5-4a84-85e4-914a3da0e7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "from jax import grad\n",
    "from jax import numpy as jnp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4adbdf9-a6c5-4a25-a896-3c47b30c66c2",
   "metadata": {},
   "source": [
    "Illustrative example: Calculate the gradient of :\n",
    "\n",
    "$$g(x) = x^2, \\qquad g'(x) = 2x$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "4d4aa2ee-f2f7-4cf6-a03e-63a2ee910a01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(xi):\n",
    "    return xi**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "b1fc2082-c8a0-42ea-9c4a-8e3fc196c2ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(6., dtype=float32, weak_type=True)"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set up the gradient function\n",
    "grad_g = grad(g) # 2x\n",
    "\n",
    "# To evaluate the gradient, need to eval at a specific point, $x$\n",
    "grad_g(3.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a46fa584-082b-4adb-8470-35b158205b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's evaluate the same parameters... just need to type cast to numpy\n",
    "param_dict_jax = {k:jnp.array(v) for k,v in param_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4eb3cc40-afca-4b55-abaa-6aa116afff68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_jax(z):\n",
    "    return jnp.where(z>0,z,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fb565620-2c7f-49ab-ac19-5eb2ae5e20d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_jax(W,X,b):\n",
    "    '''\n",
    "    Linear layer mapping H = W x + b\n",
    "    - W: array (m,n)\n",
    "    - x: array (bs, n)\n",
    "    - b: array (m)\n",
    "\n",
    "    Outputs:\n",
    "    - H: array (bs,m)\n",
    "    '''\n",
    "    \n",
    "    return jnp.einsum('mn,bn->bm',W,X) + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "e93e469d-4e10-4a94-b28f-e45ceeaad333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (X, param_dict)\n",
    "def myNN_jax(param_dict):\n",
    "\n",
    "    # Unpack the parameters\n",
    "    W1 = param_dict['W1']\n",
    "    b1 = param_dict['b1']\n",
    "    \n",
    "    W2 = param_dict['W2']\n",
    "    b2 = param_dict['b2']\n",
    "\n",
    "    # forward pass\n",
    "    z1 = linear_jax(W1, X, b1)\n",
    "    h1 = relu_jax(z1)\n",
    "\n",
    "    z2 = linear_jax(W2, h1, b2)\n",
    "    \n",
    "    return z2.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e2a3138f-7645-4dcd-a8b9-1682e79e82dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_nn = grad(myNN_jax)\n",
    "\n",
    "# And... evaluate it!\n",
    "grad_dict_jax = grad_nn(param_dict_jax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "6dba56d4-1c27-4015-995d-686ed5d1f977",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 (16, 5) (16, 5)\n",
      "W2 (1, 16) (1, 16)\n",
      "b1 (16,) (16,)\n",
      "b2 (1,) (1,)\n"
     ]
    }
   ],
   "source": [
    "for k, v in grad_dict_jax.items():\n",
    "    print(k, v.shape, grad_dict[k].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41d47fb8-5695-489c-ac1c-e8386eb7a3b4",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "bef05af7-35c9-45ff-a5fa-55be0b29a117",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k, v in grad_dict_jax.items():\n",
    "    print(k, np.all(np.isclose(v, grad_dict[k])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce891dd-bd9e-4280-af1a-49cacb077cf3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c0b09c-0039-4e48-bd96-f14c28c05207",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "68134302-7f02-4367-a1c5-f2c601fb517e",
   "metadata": {},
   "source": [
    "## Step 5: Connecting to the loss function\n",
    "\n",
    "<img src=\"../figures/hw3-Q5.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a08a8c40-16fa-455c-a7f4-10aa4657052a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "81dfe79c-c915-44da-a394-56146b5b250f",
   "metadata": {},
   "source": [
    "In theory, you can get gradients w/r.t. the full loss next, but this is where we'll stop the required homework, you've done a great job exercising your AD muscles!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd303782-2064-4251-9b4d-7017543bc0ab",
   "metadata": {},
   "source": [
    "Awesome, we've played around with auto diff (AD), coded up both some intro examples and a simple NN, and cross checked these gradients with one of the autodiff packages on the market.\n",
    "\n",
    "Tomorrow, we'll stick with these AD libraries  that do these \"gutsy\" bits for us, and move on to training, optimizers and regularization tricks. But, having this detail oriented understanding is also great for building our inution for why a lot of these tricks are motivated and \"work\", and we'll highligh that tomorrow as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bcdc14b-cdc3-45a0-924c-f96217c4ec7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
