{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1591f28-18dc-4df2-b00f-90b684adc31b",
   "metadata": {},
   "source": [
    "# Exploring the Bias Variance Tradeoff\n",
    "\n",
    "Author: Lukas Heinrich\n",
    "\n",
    "Mar 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1891c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "294969ed",
   "metadata": {},
   "source": [
    "## Step 1: Write a function `generate_data(N)` \n",
    "\n",
    "Write a function `generate_data(N)` that produces `N` samples from the following model:\n",
    "\n",
    "$$\n",
    "p(s) = p(x,y) = p(y|x)p(x)\n",
    "$$\n",
    "\n",
    "with the following \"true\" underlying polynomial noisy model\n",
    "\n",
    "$$p(x) = \\mathrm{Uniform}(-1,1)$$\n",
    "$$p(y|x) = \\mathrm{Normal}(\\mu = f(x),\\sigma = 0.2)$$\n",
    "$$f(x) = \\sum_i p_i x^i$$,\n",
    "\n",
    "with $p_0 = -0.7, p_1 = 2.2, p_2 = 0.5, p_3 = 1.0$\n",
    "\n",
    "Hint: you can use `np.polyval` to evaluate a polynomial with a fixed set of coefficients (but watch out for the order)\n",
    "\n",
    "The function should return a array of `x` values and an array of `y` values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745a45a0-ca42-49e1-b946-76d26784eb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "coeffs_true = [ ... ] # fill in\n",
    "\n",
    "def generate_data(N):\n",
    "    \n",
    "    raise NotImplementedError\n",
    "    \n",
    "    return x,y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55ce736f",
   "metadata": {},
   "source": [
    "## Step 2: Plot Samples and Functions\n",
    "\n",
    "Write a function `plot(ax, train_x, train_y, p_trained, p_true)` that\n",
    "takes a matplotlib axis object and plots\n",
    "\n",
    "* plot the true function \n",
    "* plot a second (trained or random) function \n",
    "* plot the samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53f78dc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(ax, train_x, train_y, p_trained, p_true):\n",
    "\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be203707-3cfa-4896-8ab5-ddec41590307",
   "metadata": {},
   "source": [
    "Check your function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47afa143",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = plt.figure()\n",
    "x,y = generate_data(10)\n",
    "plot(f.gca(),x,y,np.random.normal(size = (4,)), coeffs_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b901b3",
   "metadata": {},
   "source": [
    "## Step 3\n",
    "\n",
    "One can show that given a Hypothesis Set of Polynomial functions\n",
    "\n",
    "$$f(x) = \\sum_i w_i x^i$$\n",
    "\n",
    "and a risk function of the following form\n",
    "\n",
    "$$l(s) = l(x,y) = (y - f(x))^2$$\n",
    "\n",
    "there is a closed form solution for finding the empirical risk minimization, where the best fit coefficients $\\vec{w}$ is given by\n",
    "\n",
    "$$\n",
    "w = (X^T X)^{-1} X^T y\n",
    "$$\n",
    "\n",
    "where $X$ is the matrix with rows $x = (x_0,x_1,x_2,x_3,\\dots,x_d)$ and one row for each sample\n",
    "\n",
    "$$\n",
    "X = \\left(\n",
    "\\begin{array}{}\n",
    "x_0^{(1)},\\dots,x_d^{(1)}  \\\\\n",
    "x_0^{(2)},\\dots,x_d^{(2)}  \\\\\n",
    "\\dots \\\\\n",
    "x_0^{(n)},\\dots,x_d^{(n)}  \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "$$\n",
    "\n",
    "* Write a function `learn(train_x, train_y, degree)` to return the $(d+1)$ optimal coefficients for a polynomial fit of degree $d$.\n",
    "* Fit a sampled of 5 data points with degree 4\n",
    "* Plot the Trained function together with the true function using the plotting method from the last step\n",
    "* Try this multiple time to get a feel for how much the data affects the fit\n",
    "* Try degree 1 and observe how the trained function is much less sensitive to the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5898e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def learn(train_x, train_y, degree):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29eeb4a3-ea71-4d78-9e03-cfea014a7b4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eec901-1757-4105-bab7-2143bd2e7a98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1417efa7",
   "metadata": {},
   "source": [
    "## Step 4\n",
    "\n",
    "Write a function to evaluate the risk or loss of a sample. Use our loss function for which we have the training procedure above\n",
    "\n",
    "$$\n",
    "l(s) = l(x,y) = (f(x) - y)^2\n",
    "$$\n",
    "\n",
    "and right a function `risk(x,y_true, trained_coeffs)` to compute\n",
    "\n",
    "$$\n",
    "\\hat{L} = \\frac{1}{N}\\sum_i l(s_i) = \\frac{1}{N}\\sum_i l(x^{(i)},y^{(i)}) = \\frac{1}{N}\\sum_i ( f(x^{(i)}) - y^{(i)})^2\n",
    "$$\n",
    "\n",
    "* Draw a size 10 data sample and fit the result to obtain trained coefficients\n",
    "* Draw 10000 samples of size 10 and compute their empirical risk under the trained coefficients\n",
    "* Repeat the same but use the true coefficients of the underlying data-generating process\n",
    "* Histogram the two sets of 10,000 risk evaluations. Which one has lower average risk?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1307e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def risk(x, y_true, p):\n",
    "    raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baa9f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "66ec31c4",
   "metadata": {},
   "source": [
    "## Step 5\n",
    "\n",
    "Explore how the fit improves when adding more data. Plot the best fit model for data set sizes of \n",
    "\n",
    "$$N = 5,10,100,200,1000$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e2068e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "97abd1da",
   "metadata": {},
   "source": [
    "## Step 6\n",
    "\n",
    "Explore how the fit changes when using more and more complex models. Plot the best fit model for degrees\n",
    "\n",
    "$$d = 1,2,5,10$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780c82ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d20cfb13",
   "metadata": {},
   "source": [
    "## Step 7 Bias-Variance Tradeoff\n",
    "\n",
    "Draw two datasets:\n",
    "\n",
    "* A train dataset with $N=10$\n",
    "* A test dataset with $N=1000$\n",
    "\n",
    "Perform trainings on the train dataset for degrees $1\\dots8$ and store the training coefficients\n",
    "\n",
    "* Evaluate the risk under the various trainings for the train and the test dataset\n",
    "* Plot the train and test risk as a function of the polynomial degree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47343d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f8dbbb70-c13e-4b58-a7de-ced442594cc1",
   "metadata": {},
   "source": [
    "# Double Descent in Bias-Variance Tradeoff in Cubic Spline Regression\n",
    "\n",
    "A -- at first sight unintuitive  -- result in Deep Learning specifically is that we often have highly overparametrized models but that those models still generalize well.\n",
    "\n",
    "This means that the model has many more parameters than we have data points. The classic discussion\n",
    "of the bias-variance tradeoff would indicate that in such overly complex models one would expect bad generalization.\n",
    "\n",
    "The lore is that \"with enough parameters I can fit anything, no? Once I have as many parameters as data points, \n",
    "I can essentially pick the one solution that goes through all data points. This will typicall generalize horribly!\n",
    "\n",
    "But, surprisingly, the models that have even more parameters than that generalize well! How can this be?\n",
    "\n",
    "What we observe is the phenomenon of \"double descent\" - the idea that the test error will\n",
    "typically increase as we go to more and more complex models, but then - once we have more parameters\n",
    "than we have data, we enter a territory over underconstrained models. That means we have \n",
    "increasingly more modela that all perform equally well on the data (that means perfectly, since we \n",
    "can in-fact fit everything).\n",
    "\n",
    "In the over-parametrized regime, we must actively choose a model out of the family of well-performing functions.\n",
    "\n",
    "We can use this freedom to choose slighly better behaved functions, such as the \"minimum norm\" solution. I.e. the solution\n",
    "that exhibits as little \"wiggliness\" as possible. With this we can make the test error decrease again\n",
    "for overparametrized models, leading to the \"double descent\" phenomenon\n",
    "\n",
    "In a simple case without Deep Learning we can find the minimum norm solution (via singular-value decomposition) model by hand, as shown in this notebook,\n",
    "but how does it work in Deep Learning?\n",
    "\n",
    "Current research indicates that a key ingredient is the choice of optimization algorithm. Gradient Descent\n",
    "appears to have a implicit regularization property that seeks out minimum-norm solution in the manifold of \"interpolating models\"\n",
    "\n",
    "Check out the followint explainer Twitter thread by statistician Daniela Witten \n",
    "\n",
    "https://twitter.com/daniela_witten/status/1292293102103748609\n",
    "\n",
    "and the paper\n",
    "\n",
    "https://arxiv.org/abs/1912.02292\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5523718-2734-425f-a404-80e777225f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from patsy import dmatrix\n",
    "\n",
    "np.random.seed(10)\n",
    "\n",
    "def true_function(x):\n",
    "    return 3*np.sin(x)\n",
    "\n",
    "# Generate random x values in the interval [0, 10]\n",
    "x = np.random.uniform(0, 10, 15)\n",
    "y = true_function(x) + np.random.normal(0, 3.0, len(x))\n",
    "\n",
    "# Split the dataset into train and test sets (70% train, 30% test)\n",
    "split_idx = int(len(x) * 0.7)\n",
    "x_train, x_test = x[:split_idx], x[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# compute_errors\n",
    "\n",
    "print(x_train.min())\n",
    "print(x_test.min())\n",
    "print(len(x_train))\n",
    "\n",
    "def train_model(spline_basis_train, y_train, p):\n",
    "    if p + 1 < len(x_train):\n",
    "        coefficients = np.linalg.solve(spline_basis_train.T @ spline_basis_train, spline_basis_train.T @ y_train)\n",
    "    else:\n",
    "        U, s, Vt = np.linalg.svd(spline_basis_train, full_matrices=False)\n",
    "        s_inv = np.zeros_like(s)\n",
    "        s_inv[s != 0] = 1 / s[s != 0]\n",
    "        spline_basis_pinv = Vt.T @ np.diag(s_inv) @ U.T\n",
    "        coefficients = spline_basis_pinv @ y_train\n",
    "    return coefficients\n",
    "\n",
    "def compute_errors(x_train, y_train, x_test, y_test, p):\n",
    "    '''\n",
    "    TO DO: Your code here\n",
    "    '''\n",
    "    raise NotImplemented Error\n",
    "    return train_mse, test_mse\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecde6e-f838-4b5e-ad91-14aef95f4844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate train and test errors for different values of p\n",
    "p_values = np.arange(3, 2*len(x_train))\n",
    "errors = # To do, call compute_errors to calc this\n",
    "\n",
    "# Plot train and test errors as a function of p\n",
    "plt.plot(# YOUR CODE\n",
    "         label='Train MSE', marker='o')\n",
    "plt.plot(# YOUR CODE\n",
    "         label='Test MSE', marker='o')\n",
    "plt.xlabel('Number of Parameters (p)')\n",
    "plt.ylabel('Mean Squared Error (MSE)')\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "plt.title('Train and Test Errors as a Function of p')\n",
    "plt.ylim(1e-11,1e4)\n",
    "plt.semilogy()\n",
    "# plt.ylim(0,0.02)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df48466-e66f-42a1-bf93-bf8193d14c29",
   "metadata": {},
   "source": [
    "**What do you see?? Does it make sense?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34323347-2cce-4c90-bd88-09617c1575bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb61631c-1462-4470-9215-8b98f36c3f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_plot():\n",
    "    # Generate training data\n",
    "    x_train = np.random.uniform(0, 10, 15)\n",
    "    y_train = true_function(x_train) + np.random.normal(0, 3.0, len(x_train))\n",
    "\n",
    "    # Define models\n",
    "    model_p_values = [\n",
    "        2*len(x_train),  # Overparametrized model\n",
    "        len(x_train),    # Interpolation threshold model\n",
    "        len(x_train)//2  # Underparametrized model\n",
    "    ]\n",
    "\n",
    "    # Generate x values for ground truth function and fitted functions\n",
    "    x_values = np.linspace(0, 10, 1000)\n",
    "    y_true = true_function(x_values)\n",
    "\n",
    "    # Plot training data and ground truth function\n",
    "    plt.scatter(x_train, y_train, c='black', marker='o', label='Training data')\n",
    "    plt.plot(x_values, y_true, linestyle='--', label='Ground truth function')\n",
    "\n",
    "    for p in model_p_values:\n",
    "        common_min = max(x_train.min(), x_values.min())\n",
    "        common_max = min(x_train.max(), x_values.max())\n",
    "        knots = np.linspace(common_min + (common_max - common_min) * 0.01,\n",
    "                            common_max - (common_max - common_min) * 0.01, p - 2)\n",
    "        \n",
    "        spline_basis_train = dmatrix(\"cr(x, knots=knots) - 1\", {\"x\": x_train, \"knots\": knots})\n",
    "        spline_basis_values = dmatrix(\"cr(x, knots=knots) - 1\", {\"x\": x_values, \"knots\": knots})\n",
    "\n",
    "        coefficients = train_model(spline_basis_train, y_train, p)\n",
    "        y_fitted = spline_basis_values @ coefficients\n",
    "\n",
    "        plt.plot(x_values, y_fitted, label=f'Fitted function (p={p})')\n",
    "\n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.legend()\n",
    "    plt.ylim(-20,20)\n",
    "    plt.grid()\n",
    "    plt.title('Training Data, Ground Truth Function, and Fitted Functions')\n",
    "    plt.show()\n",
    "\n",
    "fit_and_plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed588831-9053-49bf-9ae4-371fed8c82d5",
   "metadata": {},
   "source": [
    "**Q:** Which model has the best fit? which has the worst fit?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26f1ac4-9609-4b2a-8f07-3eb9d3e5f67b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
